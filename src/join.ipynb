{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After the cleaning in cleaning file i will join the files togither"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0   &emsp; 5059 <br>\n",
    "2    &emsp; 394<br>\n",
    "4    &emsp; 350<br>\n",
    "1    &emsp; 282<br>\n",
    "5   &emsp;  261<br>\n",
    "3  &emsp;   184<br>\n",
    "6   &emsp;  117<br>\n",
    "7   &emsp;  115<br>\n",
    "<br>\n",
    "each day indicate how many apartment sold 0 means not sold <br>\n",
    "1 means sold in day 7/8/2022<br>\n",
    "2 means sold in day 8/8/2022 <br>\n",
    "...         ...     ...<br>\n",
    "7 means sold in day 12/8/2022<br>\n",
    "<br>\n",
    "i would like to say that i have the data in 1st , 2nd and 3rd of this month but i don't have the data in the 4th and 5th of this month so i started in the 6th \n",
    "<br>\n",
    "1/8  i have <br>\n",
    "2/8  i have <br>\n",
    "3/8  i have <br>\n",
    "4/8  i don't have <br>\n",
    "5/8  i don't have <br>\n",
    "6/8  i have <br>\n",
    "7/8  i have <br>\n",
    "8/8  i have <br>\n",
    "9/8  i have <br>\n",
    "10/8 i have <br>\n",
    "11/8 i have <br>\n",
    "12/8 i have  <br>\n",
    "13/8 i have  <br>\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import ast\n",
    "import json    \n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "yest =pd.read_csv('../data/aqar.csv') # the main which is 6-8-2022\n",
    "Today =  pd.read_csv('../data/14-8-2022,19:22.csv') #the new day file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 8 #this the day of the file from the 6th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'id', 'title', 'price', 'content', 'imgs', 'refresh', 'beds',\n",
       "       'livings', 'wc', 'area', 'street_width', 'age', 'last_update',\n",
       "       'ketchen', 'ac', 'furnished', 'location', 'path', 'user', 'district',\n",
       "       'width', 'length', 'advertiser_type', 'create_time', 'review',\n",
       "       'profileImg', 'UserName', 'iam_verified', 'rega_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Today.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "if yest.columns[0] == 'Unnamed: 0':\n",
    "    yest.drop('Unnamed: 0' , axis = 1 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Today.columns[0] == 'Unnamed: 0':\n",
    "    Today.drop('Unnamed: 0' , axis = 1 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yest['sold'] = 0 #we used this for the first time only to create the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6762, 34)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = yest.merge(Today, on='id', how='left', indicator=True) #left join that will contain the data in the main file and the data in both (in both means not sold yet)(right means pulish in after 6th)\n",
    "df3 = df3.loc[(df3['_merge'] == 'left_only') & (df3['sold'] == 0 )] # this will filter if it is both(inner ) so now i have only the left and for df3['sold'] == 0 this becouse if ot is 1 then this mean i checked it bedore and it is sold in 1 \n",
    "df3.drop(df3.filter(regex='_y$').columns, axis=1, inplace=True) # delete the extra columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'user_id_x', 'id', 'title_x', 'price_x',\n",
       "       'content_x', 'imgs_x', 'refresh_x', 'beds_x', 'livings_x', 'wc_x',\n",
       "       'area_x', 'street_width_x', 'age_x', 'last_update_x', 'ketchen_x',\n",
       "       'ac_x', 'furnished_x', 'location_x', 'path_x', 'user_x', 'district_x',\n",
       "       'width_x', 'length_x', 'advertiser_type_x', 'create_time_x', 'review_x',\n",
       "       'profileImg_x', 'UserName_x', 'iam_verified_x', 'rega_id_x', 'sold',\n",
       "       'soldAt', '_merge'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    245\n",
       "Name: sold, dtype: int64"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['sold'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['sold'] = day #this will fill all the data above with the day of it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    2\n",
       "Name: sold, dtype: int64"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yest[yest['id'] == 4554519]['sold'] #check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29    1\n",
       "Name: sold, dtype: int64"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yest[yest['id'] == 4537352]['sold'] #check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(df3['id']) : \n",
    "    yest.loc[yest[\"id\"] == i, \"sold\"] = day #now we fill the original data with day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4814\n",
       "2     394\n",
       "4     350\n",
       "1     282\n",
       "5     261\n",
       "8     245\n",
       "3     184\n",
       "6     117\n",
       "7     115\n",
       "Name: sold, dtype: int64"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yest['sold'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "yest.to_csv('../data/aqar.csv' , index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now  for the updateDate phase "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
